# AI安全研究实习生-隐私创新实验室-筋斗云人才计划

**ID**: 7477498814484842760
**Code**: A150492
**Location**: 杭州
**Category**: 算法
**Type**: 实习

## 职位描述

### 团队介绍
隐私创新实验室，致力于探索数据隐私安全领域的前沿技术和理论，为字节跳动全球业务的高速发展提供洞悉行业趋势的技术咨询和创新性的技术解决方案。隐私创新实验室在数据安全领域拥有长期愿景与决心，研究方向覆盖数字主权、合规智能、大模型个人隐私数据保护等。在日益趋严的隐私合规管控的历史时刻，多极化的数字主权意识逐步觉醒，我们更需要融合学术界和产业界的经验智慧，引入前沿技术和理论，为承载海量用户和海量数据的互联网业务提供高效完备的数据隐私安全保障，化解合规壁垒，支持业务持续突破创新。
课题介绍：生成式AI技术在创意产业、教育、医疗、法律等领域展现了巨大的潜力。然而，随着这些技术的发展，隐私问题也逐渐浮出水面。生成式AI模型通过学习大量的训练数据来生成新的内容，其中可能包含大量敏感的个人信息。如果训练数据或者模型训练过程没有进行足够的隐私保护，生成的内容可能泄露训练数据中的私人信息。例如，生成的文本可能无意中包含了训练数据中个人的敏感细节，图像生成模型可能会重构出真实世界的个人面孔或位置，甚至生成个人的生物特征。
因此，如何在不泄露个人隐私的前提下，利用生成式AI模型的强大能力，成为了一个亟待解决的关键问题。如何设计既能保证隐私保护，又能保持生成效果和模型性能的生成式AI，正成为该领域的前沿研究方向。
课题挑战：

### 实习要求


### 核心工作
1、隐私泄露风险： 生成式AI模型的训练依赖于大量的数据，尤其是在自然语言处理和图像生成领域。训练过程中，模型可能会记忆训练数据的某些特定信息，这些信息可能会被生成模型复现。举例来说，GPT类语言模型可能会无意间生成包含训练数据中某个人身份信息、地址或其他敏感数据的文本。如何确保生成模型不会泄露这些信息，成为隐私保护中的一大挑战；
2、数据扰动与模型质量： 为了防止隐私泄露，常用的隐私保护技术（如差分隐私）通常需要对训练数据进行扰动或噪声注入。然而，这种扰动可能导致生成模型失去对数据的精确建模能力，从而影响生成内容的质量。尤其在生成任务中，模型的质量直接决定了输出内容的实用性和创造性，因此，如何在保护隐私的同时，尽可能地保持生成结果的高质量，是一个亟需解决的问题；
3、模型的“记忆”与“复用”问题： 生成式AI模型通过学习大量的数据来建立生成规则，但是它们也可能在训练过程中“记住”数据的细节。这个问题在某些情况下可能表现为“记忆泄露”，即模型输出内容可能无意间重现训练集中的某些特定片段，尤其是在小样本或高敏感度的数据集上。如何防止生成式AI模型“记忆”并复用具体的个人信息，而只是学习到数据的“规律”或“特征”，是设计隐私保护机制时必须要考虑的重要问题；
4、合规性与跨境数据流动：各国对隐私保护有不同的法律规定，例如GDPR、CCPA等都对如何处理和传输个人数据提出了严格要求。对于跨境数据流动，如何确保在进行生成式AI训练时遵守不同地区的数据隐私法规，特别是在涉及敏感个人信息时，成为了一个复杂的法律和技术挑战。此外，生成式模型可能涉及多个数据源和多个国家的用户数据，如何在这些环境下平衡隐私保护与合规性，也是值得关注的问题；
5、生成内容的透明性与可解释性：尽管生成式AI模型的生成能力令人惊叹，但它们往往缺乏足够的透明性，导致用户难以理解生成结果背后的原因。在隐私保护背景下，如何使生成模型具备更好的可解释性，能够让用户理解模型是如何生成特定内容的，且该内容是否涉及隐私信息，是增强用户信任的关键。这一挑战不仅仅是技术问题，也是伦理和社会问题。

## 职位要求
1、2026届及以后毕业，博士在读，人工智能、计算机、软件、数学等相关专业优先；
2、在生成AI方向有扎实的基础和代码能力，在ICLR/NeurIPS/ICML等顶级期刊会议上发表论文者优先；
3、熟悉大模型方向的业界动态，对新知识有快速的学习好奇心和快速的上手能力；
4、良好的沟通协作能力，能和团队一起探索新技术，推进技术进步。
